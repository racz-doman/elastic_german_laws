{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.de import German\n",
    "from tqdm import tqdm\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "import ftfy as ff\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split as split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_nlp = German()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_laws = []\n",
    "prep_tr_laws = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"-f_new.txt\", encoding='utf-8') as f:\n",
    "    all_laws = f.readlines()\n",
    "all_laws = [x.strip() for x in all_laws]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "web_addresses = []\n",
    "for line in all_laws:\n",
    "    temp = line.split(' ')\n",
    "    web_addresses.append(temp[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for address in web_addresses:\n",
    "    temp = address.split('/')\n",
    "    words.append(temp[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = set()\n",
    "for word in words:\n",
    "    category.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lawJson = json.loads(all_laws[1])\n",
    "print(lawJson[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_data = shuffle(all_laws, random_state=7)\n",
    "print(shuffled_data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_stopwords = [']','[','{','}','.',',',':','#','-','\"','!','?','*','&','@','˝',')','(',';','´',' ','/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(s):\n",
    "    str1 = \" \"\n",
    "    return (str1.join(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(sentence):\n",
    "    temp = de_nlp(sentence)\n",
    "    words = []\n",
    "    for j, token in enumerate(temp):\n",
    "        if not token.is_stop and token.text not in other_stopwords:\n",
    "             words.append((token.lemma_))\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_law = []\n",
    "for i, var in tqdm(enumerate(all_laws), 'Token and Lemmatization'):\n",
    "    if len(var) > 1000000:\n",
    "        continue\n",
    "    temp = json.loads(var)\n",
    "    final = []\n",
    "    categories = temp[\"file\"].split(\"/\") \n",
    "    final.append(categories[4])\n",
    "    if \"text\" in temp:\n",
    "        if \"Kurztitel\" in temp[\"text\"]:\n",
    "            final.append(lemmatize(listToString(temp[\"text\"][\"Kurztitel\"])))\n",
    "        else:\n",
    "            final.append(\"Missing\")\n",
    "        if \"Kundmachungsorgan\" in temp[\"text\"]:\n",
    "            final.append(lemmatize(listToString(temp[\"text\"][\"Kundmachungsorgan\"])))\n",
    "        else:\n",
    "            final.append(\"Missing\")\n",
    "        if \"Beachte\" in temp[\"text\"]:\n",
    "            final.append(lemmatize(listToString(temp[\"text\"][\"Beachte\"])))\n",
    "        else:\n",
    "            final.append(\"Missing\")\n",
    "        final.append(lemmatize(listToString(temp[\"text\"])))    \n",
    "    else:\n",
    "        final.append(\"Missing\")\n",
    "        final.append(\"Missing\")\n",
    "        final.append(\"Missing\")\n",
    "        final.append(lemmatize(var))\n",
    "    all_data_law.append(final)\n",
    "df = pd.DataFrame(all_data_law,columns=['Category','Kurztitel','Kundmachungsorgan','Beachte','All_data'])\n",
    "#df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Category\",\"Kurztitel\",\"Kundmachungsorgan\",\"Beachte\",\"All_data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for element in category:\n",
    "    print(element + \" :\" + str(len(category_df[category_df.Category == element])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_freq(s):\n",
    "    freq = defaultdict(int)\n",
    "    for word in s:\n",
    "        freq[word.lower()] += 1\n",
    "    return freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterOnDatas(df):\n",
    "    big_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        for element in row[\"All_data\"]:\n",
    "            big_list.append(element)\n",
    "    return big_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordsfreq = word_freq(iterOnDatas(df))\n",
    "sorted_words = sorted(wordsfreq.items(), key=lambda kv: kv[1])\n",
    "sorted_words.reverse()\n",
    "sorted_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df[(df.Category != \"Erlaesse\") & (df.Category != \"LgblNO\")]\n",
    "new_df[[\"Category\",\"Kurztitel\",\"Kundmachungsorgan\",\"Beachte\",\"All_data\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i, var in tqdm(enumerate(all_laws), 'Token and Lemmatization'):\n",
    "    if len(var) > 1000000:\n",
    "        continue\n",
    "    final = []\n",
    "    categories = var.split('/')\n",
    "    final.append(categories[4])\n",
    "    final.append(var)\n",
    "    final.append(len(var))\n",
    "    final.append(len(var.split(\" \")))\n",
    "    data_list.append(final)\n",
    "category_df = pd.DataFrame(data_list,columns=['Category','All_data','Length','Words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter laws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for i, var in tqdm(enumerate(all_laws), 'Token and Lemmatization'):\n",
    "    if len(var) > 1000000:\n",
    "        continue\n",
    "    final = []\n",
    "    categories = var.split('/')\n",
    "    final.append(categories[4])\n",
    "    final.append(var)\n",
    "    data_list.append(final)\n",
    "category_df = pd.DataFrame(data_list,columns=['Category','All_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df = category_df[(category_df.Category != \"Erlaesse\") & (category_df.Category != \"LgblNO\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max length: \" + str(category_df.Length.max()))\n",
    "print(\"Most words: \" + str(category_df.Words.max()))\n",
    "print(\"Min length: \" + str(category_df.Length.min()))\n",
    "print(\"Least words: \" + str(category_df.Words.min()))\n",
    "print(\"Length mean: \" + str(category_df.Length.mean()))\n",
    "print(\"Words mean: \" + str(category_df.Words.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "category_df[category_df.Words == 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_df.loc[123895][\"All_data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_category_df = category_df[(category_df.Words > 21)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max length: \" + str(new_category_df.Length.max()))\n",
    "print(\"Most words: \" + str(new_category_df.Words.max()))\n",
    "print(\"Min length: \" + str(new_category_df.Length.min()))\n",
    "print(\"Least words: \" + str(new_category_df.Words.min()))\n",
    "print(\"Length mean: \" + str(new_category_df.Length.mean()))\n",
    "print(\"Words mean: \" + str(new_category_df.Words.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_category = []\n",
    "all_data = []\n",
    "for i, row in new_category_df.iterrows():\n",
    "    all_category.append(row[\"Category\"])\n",
    "    all_data.append(row[\"All_data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data,tst_data,tr_labels,tst_labels = split(all_data,all_category,test_size=0.2,random_state=20,stratify=all_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data,dev_data,tr_labels,dev_labels = split(tr_data,tr_labels,test_size=0.2,random_state=20,stratify=tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"tr_data_with_category.txt\", \"w\", encoding=\"utf-8\")\n",
    "fff = open(\"tst_data_with_category.txt\",\"w\", encoding=\"utf-8\")\n",
    "ffff = open(\"dev_data_with_category.txt\",\"w\", encoding=\"utf-8\")\n",
    "for i, row in enumerate(tr_data):\n",
    "    temp = '{\"ID\": ' + str(i) +', \"Data\": ' + row + ', \"Label\": \"' + tr_labels[i] +'\"}' + \"\\n\"\n",
    "    f.write(temp)\n",
    "for i, row in enumerate(tst_data):\n",
    "    temp = '{\"ID\": ' + str(i) +', \"Data\": ' + row + ', \"Label\": \"' + tst_labels[i] +'\"}' + \"\\n\"\n",
    "    fff.write(temp)\n",
    "for i, row in enumerate(dev_data):\n",
    "    temp = '{\"ID\": ' + str(i) +', \"Data\": ' + row + ', \"Label\": \"' + dev_labels[i] +'\"}' + \"\\n\"\n",
    "    ffff.write(temp)\n",
    "f.close()\n",
    "fff.close()\n",
    "ffff.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
